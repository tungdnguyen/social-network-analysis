{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- iterrows(): iterate through the value each row of the DF\n",
    "- https://stackoverflow.com/questions/36684013/extract-column-value-based-on-another-column-pandas-dataframe: used in make_prediction function. Can't seem to use .query() since it returns a series object\n",
    "- shape(): returns tuple (row,column) for the dataframe\n",
    "- To iterate over rows in df, use either zip() or itertuples() since this is much faster than iterrows()\n",
    "- USING ZIP() FOR THE 2 FOR LOOPS OPTIMIZED THE CODE FROM 2MINS TO 9 SEC\n",
    "- isin() function is to query df based on multiple values\n",
    "- dot product of csr_matrix is much slower than dot product of normal matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from unittest import TestCase\n",
    "def test_vectorize_2():\n",
    "    tc = TestCase()\n",
    "    movies = pd.DataFrame([[123, 'horror|horror|romance|romance|romance',\n",
    "                                 ['horror', 'horror', 'romance', 'romance', 'romance']],\n",
    "                                [456, 'romance', ['romance']]], columns=['movieId', 'genres', 'tokens'])\n",
    "    movies, vocab = featurize(movies)\n",
    "    row0 = movies['features'].tolist()[0]\n",
    "    tc.assertEqual(round(max(list(row0.data)), 2),\n",
    "                        0.20)\n",
    "    tc.assertEqual(round(min(row0.toarray()[0]), 1),\n",
    "                        0.0)\n",
    "test_vectorize_2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.3692057132720947\n",
      "vocab:\n",
      "[('action', 0), ('adventure', 1), ('animation', 2), ('children', 3), ('comedy', 4), ('crime', 5), ('documentary', 6), ('drama', 7), ('fantasy', 8), ('film-noir', 9)]\n",
      "99903 training ratings; 101 testing ratings\n",
      "error=0.787455\n",
      "[2.7945653  2.62385286 2.76558239 4.24064843 3.22115155 4.11423684\n",
      " 3.92040563 3.98286924 3.21565818 3.32692774]\n"
     ]
    }
   ],
   "source": [
    "# coding: utf-8\n",
    "\n",
    "# # Assignment 3:  Recommendation systems\n",
    "#\n",
    "# Here we'll implement a content-based recommendation algorithm.\n",
    "# It will use the list of genres for a movie as the content.\n",
    "# The data come from the MovieLens project: http://grouplens.org/datasets/movielens/\n",
    "# Note that I have not provided many doctests for this one. I strongly\n",
    "# recommend that you write your own for each function to ensure your\n",
    "# implementation is correct.\n",
    "\n",
    "# Please only use these imports.\n",
    "from collections import Counter, defaultdict\n",
    "import math\n",
    "import numpy as np\n",
    "import os\n",
    "import pandas as pd\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "import urllib.request\n",
    "import zipfile\n",
    "import time\n",
    "def download_data():\n",
    "    \"\"\" DONE. Download and unzip data.\n",
    "    \"\"\"\n",
    "    url = 'https://www.dropbox.com/s/p9wmkvbqt1xr6lc/ml-latest-small.zip?dl=1'\n",
    "    urllib.request.urlretrieve(url, 'ml-latest-small.zip')\n",
    "    zfile = zipfile.ZipFile('ml-latest-small.zip')\n",
    "    zfile.extractall()\n",
    "    zfile.close()\n",
    "\n",
    "\n",
    "def tokenize_string(my_string):\n",
    "    \"\"\" DONE. You should use this in your tokenize function.\n",
    "    \"\"\"\n",
    "    return re.findall('[\\w\\-]+', my_string.lower())\n",
    "\n",
    "\n",
    "def tokenize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'tokens'.\n",
    "    This will contain a list of strings, one per token, extracted\n",
    "    from the 'genre' field of each movie. Use the tokenize_string method above.\n",
    "unique_token.extend(list(set(token_list)))unique_token.extend(list(set(token_list)))unique_token.extend(list(set(token_list)))\n",
    "    Note: you may modify the movies parameter directly; no need to make\n",
    "    a new copy.\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      The movies DataFrame, augmented to include a new column called 'tokens'.\n",
    "\n",
    "    >>> movies = pd.DataFrame([[123, 'Horror|Romance'], [456, 'Sci-Fi']], columns=['movieId', 'genres'])\n",
    "    >>> movies = tokenize(movies)\n",
    "    >>> movies['tokens'].tolist()\n",
    "    [['horror', 'romance'], ['sci-fi']]\n",
    "    \"\"\"\n",
    "    token = []\n",
    "    #for genres in movies['genres']:\n",
    "        #token.append(tokenize_string(genres))\n",
    "    #movies = movies.assign(tokens=pd.Series(token,movies.index))\n",
    "    tokens = lambda x: tokenize_string(x)\n",
    "    movies = movies.assign(tokens=pd.Series(movies['genres'].apply(tokens),movies.index))\n",
    "    return movies\n",
    "\n",
    "\n",
    "def featurize(movies):\n",
    "    \"\"\"\n",
    "    Append a new column to the movies DataFrame with header 'features'.\n",
    "    Each row will contain a csr_matrix of shape (1, num_features). Each\n",
    "    entry in this matrix will contain the tf-idf value of the term, as\n",
    "    defined in class:\n",
    "    tfidf(i, d) := tf(i, d) / max_k tf(k, d) * log10(N/df(i))\n",
    "    where:\n",
    "    i is a term\n",
    "    d is a document (movie)\n",
    "    tf(i, d) is the frequency of term i in document d\n",
    "    max_k tf(k, d) is the maximum frequency of any term in document d\n",
    "    N is the number of documents (movies)\n",
    "    df(i) is the number of unique documents containing term i\n",
    "\n",
    "    Params:\n",
    "      movies...The movies DataFrame\n",
    "    Returns:\n",
    "      A tuple containing:\n",
    "      - The movies DataFrame, which has been modified to include a column named 'features'.\n",
    "      - The vocab, a dict from term to int. Make sure the vocab is sorted alphabetically as in a2 (e.g., {'aardvark': 0, 'boy': 1, ...})\n",
    "    \"\"\"\n",
    "    tf_list = []\n",
    "    #Contain unique term for each movie, Ex: ['horror','horror'] => ['horror']\n",
    "    unique_token = []\n",
    "    #Get all the tf(i,d) of each term i for each document d\n",
    "    for token_list in movies['tokens']:\n",
    "        tf = defaultdict(lambda: 0)\n",
    "        for token in token_list:\n",
    "            tf[token] += 1\n",
    "        tf_list.append(tf)\n",
    "        unique_token.extend(list(set(token_list)))\n",
    "        \n",
    "    #Get all df(i) for each term i\n",
    "    df = Counter(unique_token)\n",
    "    unique_token = sorted(set(unique_token))\n",
    "    vocab = defaultdict(lambda:len(vocab))\n",
    "    for token in unique_token:\n",
    "        vocab[token]\n",
    "    \n",
    "    #Constructing csr matrix for each row\n",
    "    N = movies.shape[0]\n",
    "    matrix_list = []\n",
    "    for i in range(N):\n",
    "        tf = tf_list[i]\n",
    "        X = np.zeros((1,len(vocab)))\n",
    "        max_k = max(tf.values())\n",
    "        #Insert each term and its value into the matrix in sorted order\n",
    "        terms = sorted(tf.keys())\n",
    "        for term in vocab:\n",
    "            if term in terms:\n",
    "                j = vocab[term]\n",
    "                X[0,j] = tf[term]/max_k * np.log10(N/df[term])\n",
    "                \n",
    "        matrix_list.append(csr_matrix(X))\n",
    "    \n",
    "    movies = movies.assign(features=pd.Series(matrix_list,movies.index))\n",
    "    \n",
    "    return movies, dict(vocab)\n",
    "        \n",
    "\n",
    "\n",
    "def train_test_split(ratings):\n",
    "    \"\"\"DONE.\n",
    "    Returns a random split of the ratings matrix into a training and testing set.\n",
    "    \"\"\"\n",
    "    test = set(range(len(ratings))[::1000])\n",
    "    train = sorted(set(range(len(ratings))) - test)\n",
    "    test = sorted(test)\n",
    "    return ratings.iloc[train], ratings.iloc[test]\n",
    "\n",
    "\n",
    "def cosine_sim(a, b):\n",
    "    \"\"\"\n",
    "    Compute the cosine similarity between two 1-d csr_matrices.\n",
    "    Each matrix represents the tf-idf feature vector of a movie.\n",
    "    Params:\n",
    "      a...A csr_matrix with shape (1, number_features)\n",
    "      b...A csr_matrix with shape (1, number_features)\n",
    "    Returns:\n",
    "      The cosine similarity, defined as: dot(a, b) / ||a|| * ||b||\n",
    "      where ||a|| indicates the Euclidean norm (aka L2 norm) of vector a.\n",
    "    \"\"\"\n",
    "    a = a.toarray()[0]\n",
    "    b = b.toarray()[0]\n",
    "    return np.dot(a,b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "\n",
    "\n",
    "def make_predictions(movies, ratings_train, ratings_test):\n",
    "    \"\"\"\n",
    "    Using the ratings in ratings_train, predict the ratings for each\n",
    "    row in ratings_test.\n",
    "\n",
    "    To predict the rating of user u for movie i: Compute the weighted average\n",
    "    rating for every other movie that u has rated.  Restrict this weighted\n",
    "    average to movies that have a positive cosine similarity with movie\n",
    "    i. The weight for movie m corresponds to the cosine similarity between m\n",
    "    and i.\n",
    "\n",
    "    If there are no other movies with positive cosine similarity to use in the\n",
    "    prediction, use the mean rating of the target user in ratings_train as the\n",
    "    prediction.\n",
    "\n",
    "    Params:\n",
    "      movies..........The movies DataFrame.\n",
    "      ratings_train...The subset of ratings used for making predictions. These are the \"historical\" data.\n",
    "      ratings_test....The subset of ratings that need to predicted. These are the \"future\" data.\n",
    "    Returns:\n",
    "      A numpy array containing one predicted rating for each element of ratings_test.\n",
    "    \"\"\"\n",
    "    ratings = []\n",
    "    for user, movie in zip(ratings_test['userId'],ratings_test['movieId']):\n",
    "        #Take all the movies user with userid \"user\" rated\n",
    "        ratings_per_user = ratings_train[ratings_train.userId == user]\n",
    "        csr_1 = movies.loc[movies['movieId']==movie, 'features'].iloc[0]\n",
    "        ratings_per_user = ratings_per_user.join(movies.set_index('movieId'),on = 'movieId',how = 'inner')\n",
    "        weighted_total = 0\n",
    "        sum_cos = 0\n",
    "        for movieId, rating, csr_matrix_feature in zip(ratings_per_user['movieId'],ratings_per_user['rating'],ratings_per_user['features']):\n",
    "            cos_sim = cosine_sim(csr_matrix_feature,csr_1)\n",
    "            if cos_sim > 0:\n",
    "                weighted_total += cos_sim * rating\n",
    "                sum_cos+=cos_sim\n",
    "        if sum_cos > 0:\n",
    "            ratings.append(weighted_total/sum_cos)\n",
    "        elif sum_cos == 0:\n",
    "            ratings.append(ratings_per_user['rating'].mean())\n",
    "    return np.array(ratings)\n",
    "            \n",
    "\n",
    "def mean_absolute_error(predictions, ratings_test):\n",
    "    \"\"\"DONE.\n",
    "    Return the mean absolute error of the predictions.\n",
    "    \"\"\"\n",
    "    return np.abs(predictions - np.array(ratings_test.rating)).mean()\n",
    "\n",
    "import time\n",
    "def main():\n",
    "    download_data()\n",
    "    path = 'ml-latest-small'\n",
    "    ratings = pd.read_csv(path + os.path.sep + 'ratings.csv')\n",
    "    movies = pd.read_csv(path + os.path.sep + 'movies.csv')\n",
    "    movies = tokenize(movies)\n",
    "    start_time = time.time()\n",
    "    movies, vocab = featurize(movies)\n",
    "    print(str(time.time() - start_time))\n",
    "    print('vocab:')\n",
    "    print(sorted(vocab.items())[:10])\n",
    "    ratings_train, ratings_test = train_test_split(ratings)\n",
    "    print('%d training ratings; %d testing ratings' % (len(ratings_train), len(ratings_test)))\n",
    "    predictions = make_predictions(movies, ratings_train, ratings_test)\n",
    "    print('error=%f' % mean_absolute_error(predictions, ratings_test))\n",
    "    print(predictions[:10])\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
