{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "unhashable type: 'numpy.ndarray'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-53-41f1fa716ad6>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'c'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0marray\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'a'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'b'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0ma\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0ma\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: unhashable type: 'numpy.ndarray'"
     ]
    }
   ],
   "source": [
    "import numpy.linalg "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reading and labeling raw tweets...\n",
      "\n",
      "Sampling train/test set using Stratified Sampling...\n",
      "\n",
      "Evaluating over multiple preprocessing settings...\n",
      "\n",
      "Best cross-validation result:\n",
      "{'min_freq': 3, 'mention': False, 'accuracy': 0.86041666666666683, 'features': (<function token_features at 0x0000023462B63C80>,), 'punct': False, 'url': False}\n",
      "Worst cross-validation result:\n",
      "{'min_freq': 1, 'mention': False, 'accuracy': 0.60416666666666663, 'features': (<function token_pair_features at 0x00000234654C5AE8>,), 'punct': True, 'url': False}\n",
      "\n",
      "Mean Accuracy per setting: \n",
      "min_freq=6: 0.81701\n",
      "min_freq=5: 0.81476\n",
      "features=token_features : 0.81308\n",
      "min_freq=3: 0.81155\n",
      "min_freq=4: 0.81146\n",
      "collapse_mention=False: 0.81046\n",
      "min_freq=7: 0.81007\n",
      "punct=False: 0.80874\n",
      "min_freq=2: 0.80521\n",
      "collapse_url=True: 0.80201\n",
      "features=token_features token_pair_features : 0.80055\n",
      "min_freq=8: 0.80052\n",
      "collapse_url=False: 0.79813\n",
      "min_freq=9: 0.79236\n",
      "punct=True: 0.79140\n",
      "collapse_mention=True: 0.78968\n",
      "features=token_pair_features : 0.78657\n",
      "min_freq=1: 0.73767\n",
      "\n",
      "Fine-tuning Multinomial Naive Bayes...\n",
      "Accuracy of fine-tuned MNB on train set: 0.9020833333333333\n",
      "Accuracy of fine-tuned MNB on test set: 0.8416666666666667\n",
      "\n",
      "TOP COEFFICIENTS PER CLASS:\n",
      "Top words used by female critic:\n",
      "token=rodgers: 6.81328\n",
      "token=nikki: 6.81328\n",
      "token=nfl: 6.81328\n",
      "token=segaloff: 6.81328\n",
      "token=gb: 6.81328\n",
      "token=nat: 6.81328\n",
      "token=congress: 6.81328\n",
      "token=shape: 6.81328\n",
      "token=nails: 6.81328\n",
      "token=short: 6.81328\n",
      "\\Top words used by male critic\n",
      "token=co: -2.27737\n",
      "token=https: -2.27737\n",
      "token=movie: -4.88959\n",
      "token=amp: -4.93557\n",
      "token=new: -4.93557\n",
      "token=league: -5.08777\n",
      "token=one: -5.14412\n",
      "token=see: -5.20384\n",
      "token=movies: -5.20384\n",
      "token=justice: -5.20384\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "classify.py\n",
    "\"\"\"\n",
    "# coding: utf-8\n",
    "from collections import Counter, defaultdict\n",
    "from itertools import chain, combinations\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import os\n",
    "import re\n",
    "from scipy.sparse import csr_matrix\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "import string\n",
    "import tarfile\n",
    "import urllib.request\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import sentiwordnet as swn\n",
    "import pickle\n",
    "import json\n",
    "import random\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "stop_word = set(stopwords.words('english'))\n",
    "\n",
    "def get_tweet_data(filename):\n",
    "    with open(filename,'r') as f:\n",
    "        tweets = json.load(f)\n",
    "    return tweets\n",
    "\n",
    "def filter_tweets(tweets):\n",
    "    for tweet in tweets:\n",
    "        #Get rid of links\n",
    "        tweet = re.sub('http\\S+',' ',tweet)\n",
    "        #Get rid of mentions\n",
    "        tweet = re.sub('@\\S+',' ',tweet)\n",
    "    return tweet\n",
    "\n",
    "\n",
    "def tokenize(tweet, keep_internal_punct=True, collapse_url = True, collapse_mention = True):\n",
    "    tweet = tweet.lower()\n",
    "    if collapse_url:\n",
    "        tweet = re.sub('http\\S+','THIS_IS_URL',tweet)\n",
    "    if collapse_mention:\n",
    "        tweet = re.sub('@\\S+','THIS_IS_MENTION',tweet)\n",
    "    if not keep_internal_punct:\n",
    "        tweet = re.sub('\\W+',' ',tweet).split()\n",
    "    else:\n",
    "        tweet = re.findall('\\w[^\\s]*\\w|\\w+', tweet)\n",
    "    return np.array(tweet)\n",
    "\n",
    "#Shuffle to get random sample of data\n",
    "def shuffle_two_list(list1,list2):\n",
    "    c = list(zip(list1,list2))\n",
    "    random.shuffle(c)\n",
    "    list1, list2 = zip(*c)\n",
    "    return list1,list2\n",
    "\n",
    "def assign_label_to_tweet(male_tweets,female_tweets):\n",
    "    tweets = []\n",
    "    labels = []\n",
    "    for tweet in male_tweets:\n",
    "        tweets.append(tweet)\n",
    "        labels.append(1)\n",
    "    for tweet in female_tweets:\n",
    "        tweets.append(tweet)\n",
    "        labels.append(0)\n",
    "    tweets, labels = shuffle_two_list(tweets,labels)\n",
    "    return np.array(tweets), np.array(labels)\n",
    "\n",
    "def get_train_test(tweets,labels):\n",
    "    stratSplit = StratifiedShuffleSplit(n_splits = 3, test_size = 0.2,random_state = 42)\n",
    "    for train_index, test_index in stratSplit.split(tweets,labels):\n",
    "        X_train, X_test = tweets[train_index], tweets[test_index]\n",
    "        y_train, y_test = labels[train_index], labels[test_index]\n",
    "        \n",
    "    return X_train,X_test, y_train, y_test\n",
    "\n",
    "def token_features(tokens, feats):\n",
    "    for token in tokens:\n",
    "        if token not in stop_word:\n",
    "            feats['token='+token]+=1\n",
    "\n",
    "\n",
    "def token_pair_features(tokens, feats, k=5):\n",
    "    token_pair = []\n",
    "    for i in range(len(tokens)):\n",
    "        token_nearby = tokens[i:(i+k)]\n",
    "        if len(token_nearby) == k:\n",
    "            temp = list(combinations(token_nearby,2))\n",
    "            for token1, token2 in temp:\n",
    "                if token1 not in stop_word or token2 not in stop_word:\n",
    "                    #token_pair is now a list of tuple\n",
    "                    token_pair.append((token1,token2))\n",
    "        elif len(token_nearby) < k:\n",
    "            break\n",
    "            \n",
    "    for token1, token2 in token_pair:\n",
    "        feats['token_pair='+token1.lower()+'__'+token2.lower()] += 1\n",
    "\n",
    "neg_words = ['bad', 'hate', 'horrible', 'worst', 'boring']\n",
    "pos_words = ['awesome', 'amazing', 'best', 'good', 'great', 'love', 'wonderful']\n",
    "def lexicon_features(tokens, feats):\n",
    "    #Initialize so (neg_words,0) or (pos_words,0) also appear in feats\n",
    "    feats['pos_words'] = 0\n",
    "    feats['neg_words'] = 0\n",
    "    for token in tokens:\n",
    "        if token.lower() in pos_words:\n",
    "            feats['pos_words']+=1\n",
    "        if token.lower() in neg_words:\n",
    "            feats['neg_words'] +=1\n",
    "\n",
    "def featurize(tokens, feature_fns):\n",
    "    feats = defaultdict(lambda: 0)\n",
    "    for func in feature_fns:\n",
    "        func(tokens,feats)\n",
    "    return sorted(feats.items(),key = lambda x: x[0])\n",
    "\n",
    "\n",
    "def vectorize(tokens_list, feature_fns, min_freq, vocab=None):\n",
    "    \"\"\"CREATING VOCAB AND CONSTRICTING FEATURES TO HAVE AT LEAST MIN_FREQ APPEARANCE\"\"\"\n",
    "    #List of list, first list corresponds to the features of doc 1, second to doc 2, so on\n",
    "    feats = []\n",
    "    #To get all the possible terms for the vocab dict and able to universally sort them\n",
    "    feats_list = []\n",
    "    temp_list = []\n",
    "    for tokens in tokens_list:\n",
    "        features = featurize(tokens,feature_fns)\n",
    "        #each of the sublist corresponds to the feature of a document\n",
    "        feats.append(features)\n",
    "        feats_list.extend(features)\n",
    "    feats_list = sorted(feats_list,key = lambda x: x[0])\n",
    "    \n",
    "    #If vocab is None, then start filling the vocab dict\n",
    "    if not vocab:\n",
    "        vocab = defaultdict(lambda : len(vocab))\n",
    "\n",
    "        for tup in feats_list:\n",
    "            temp_list.append(tup[0])\n",
    "        #count contains term : number of doc it appears in\n",
    "        count = Counter(temp_list)\n",
    "        for tup in feats_list:\n",
    "            if count[tup[0]] >= min_freq:\n",
    "                #looking up a term, defaultdict will assign an incremental value to the term for us\n",
    "                vocab[tup[0]]\n",
    "    \n",
    "    \"\"\"CREATING CSR\"\"\"\n",
    "    X = np.zeros((len(tokens_list),len(vocab)))\n",
    "    for i, tokens in enumerate(feats):\n",
    "        for token in tokens:\n",
    "            #If the term is in vocab, which means it appears in at least min_freq docs\n",
    "            if token[0] in vocab:\n",
    "                #get the column where the token belongs\n",
    "                j = vocab[token[0]]\n",
    "                X[i,j] = token[1]\n",
    "    #X = csr_matrix(X)\n",
    "    return X, vocab\n",
    "    \n",
    "    \n",
    "\n",
    "def accuracy_score(truth, predicted):\n",
    "    return len(np.where(truth==predicted)[0]) / len(truth)\n",
    "\n",
    "\n",
    "def cross_validation_accuracy(clf, X, labels, k):\n",
    "    cv = KFold(n_splits = k)\n",
    "    accuracies = []\n",
    "    for train_index, test_index in cv.split(X):\n",
    "        clf.fit(X[train_index],labels[train_index])\n",
    "        predicted = clf.predict(X[test_index])\n",
    "        accuracies.append(accuracy_score(labels[test_index],predicted))\n",
    "    return np.mean(accuracies)\n",
    "                    \n",
    "\n",
    "\n",
    "def eval_all_combinations(docs, labels, punct_vals, collapse_urls, collapse_mentions,\n",
    "                          feature_fns, min_freqs):\n",
    "    result = []\n",
    "    for value in punct_vals:\n",
    "        for value1 in collapse_urls:\n",
    "            for value2 in collapse_mentions:\n",
    "                tokens_list = [tokenize(d,keep_internal_punct = value, collapse_url = value1, collapse_mention = value2) for d in docs]\n",
    "                for min_freq in min_freqs:\n",
    "                    for i in range(1,len(feature_fns)+1):\n",
    "                        #Feature lists is a list of tuple for each value of i\n",
    "                        feature_lists = combinations(feature_fns,i)\n",
    "                        for feature_list in feature_lists:\n",
    "                            feature_list = list(feature_list)\n",
    "                            X, vocab = vectorize(tokens_list,feature_list,min_freq)\n",
    "                            avg = cross_validation_accuracy(MultinomialNB(),\n",
    "                                                            X,labels,5)\n",
    "                            result.append({\n",
    "                                    'features':tuple(feature_list),\n",
    "                                    'punct':value,\n",
    "                                    'url':value1,\n",
    "                                    'mention':value2,\n",
    "                                    'accuracy':avg,\n",
    "                                    'min_freq':min_freq\n",
    "                                })\n",
    "    result = sorted(result,key = lambda x: (x['accuracy'],x['min_freq']), reverse = True)\n",
    "    return result\n",
    "            \n",
    "def plot_sorted_accuracies(results):\n",
    "    results = sorted(results, key = lambda x: (x['accuracy'],x['min_freq']))\n",
    "    plt.plot([i for i in range(len(results))],[result['accuracy'] for result in results],'b-')\n",
    "    plt.xlabel('Setting')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.savefig('accuracies.png')\n",
    "\n",
    "\n",
    "def mean_accuracy_per_setting(results):\n",
    "    mean_acc = []\n",
    "    setting_to_acc = defaultdict(list)\n",
    "    for result in results:\n",
    "        acc = result['accuracy']\n",
    "        if result['punct'] == True:\n",
    "            setting_to_acc['punct=True'].append(acc)\n",
    "        if not result['punct']:\n",
    "            setting_to_acc['punct=False'].append(acc)\n",
    "        setting_to_acc['min_freq='+str(result['min_freq'])].append(acc)\n",
    "        setting_to_acc['collapse_url='+str(result['url'])].append(acc)\n",
    "        setting_to_acc['collapse_mention='+str(result['mention'])].append(acc)\n",
    "        string = 'features='\n",
    "        for feature in result['features']:\n",
    "            string += feature.__name__ + ' '\n",
    "        setting_to_acc[string].append(acc)\n",
    "            \n",
    "    for setting, accuracies in setting_to_acc.items():\n",
    "        mean_acc.append((np.mean(accuracies),setting))\n",
    "    \n",
    "    return sorted(mean_acc,key = lambda x: x[0], reverse = True)\n",
    "    \n",
    "            \n",
    "\n",
    "def fit_best_classifier(docs, labels, best_result,best_params):\n",
    "    tokens_list = [tokenize(d,keep_internal_punct = best_result['punct'],collapse_url = best_result['url'],\n",
    "                           collapse_mention = best_result['mention']) for d in docs]\n",
    "    X, vocab = vectorize(tokens_list,list(best_result['features']),best_result['min_freq'])\n",
    "    clf = MultinomialNB(fit_prior = best_params['fit_prior'],alpha = best_params['alpha'])\n",
    "    clf.fit(X,labels)\n",
    "    return clf, vocab,clf.predict(X)\n",
    "\n",
    "def predict_test_data(clf,docs,best_result,vocab):\n",
    "    tokens_list = [tokenize(d,keep_internal_punct = best_result['punct'],collapse_url = best_result['url'],\n",
    "                           collapse_mention = best_result['mention']) for d in docs]\n",
    "    X, vocab = vectorize(tokens_list,list(best_result['features']),best_result['min_freq'],vocab)\n",
    "    return clf.predict(X)\n",
    "\n",
    "def print_top_positive(tweets,X_test,clf,n):\n",
    "    prediction = clf.predict(X_test)\n",
    "    probs = clf.predict_proba(X_test)\n",
    "    probs = [prob[1] for prob in probs]\n",
    "    top = []\n",
    "    for tweet, probability in zip(tweets,probs):\n",
    "        top.append({\n",
    "                'tweet': tweet,\n",
    "                'prediction': 1,\n",
    "                'proba': round(probability,4)\n",
    "                })\n",
    "    top = sorted(top,key = lambda x: x['proba'],reverse = True)\n",
    "    for i in range(n):\n",
    "        info = top[i]\n",
    "        print('Prediction = {} probability = {}'.format(1,info['proba'])+'\\t\\t\\t'+info['tweet'])\n",
    "\n",
    "def print_top_misclassified(test_docs, test_labels, X_test, clf, n):\n",
    "    predicted = clf.predict(X_test)\n",
    "    prob = clf.predict_proba(X_test)\n",
    "    incorrect = []\n",
    "    for doc, label, prediction, probability in zip(test_docs,test_labels,predicted,prob):\n",
    "        if label != prediction:\n",
    "            if prediction == 0:\n",
    "                incorrect.append({\n",
    "                        'doc':doc,\n",
    "                        'truth':label,\n",
    "                        'predicted':prediction,\n",
    "                        'proba': round(probability[0],6)\n",
    "                    })\n",
    "            elif prediction == 1:\n",
    "                incorrect.append({\n",
    "                        'doc':doc,\n",
    "                        'truth':label,\n",
    "                        'predicted':prediction,\n",
    "                        'proba': round(probability[1],6)\n",
    "                    })\n",
    "    incorrect = sorted(incorrect, key = lambda x: x['proba'], reverse = True)\n",
    "    for i in range(n):\n",
    "        info = incorrect[i]\n",
    "        print('\\n')\n",
    "        print('truth={} predicted={} proba={}'.format(info['truth'],info['predicted'],info['proba']))\n",
    "        print(info['doc'])\n",
    "\n",
    "def top_coefs(clf, label, n, vocab):\n",
    "    vocab = sorted(vocab.items(), key = lambda x: x[1])\n",
    "    term_vocab = np.array([tup[0] for tup in vocab])\n",
    "    if label == 0:\n",
    "        coef = clf.coef_[0]\n",
    "        top_idx = np.argsort(coef)[:n]\n",
    "        top_coef = coef[top_idx]\n",
    "    else:\n",
    "        coef = clf.coef_[0]\n",
    "        top_idx = np.argsort(coef)[::-1][:n]\n",
    "        top_coef = coef[top_idx]\n",
    "        \n",
    "    \n",
    "    top_term = term_vocab[top_idx]\n",
    "    return [x for x in zip(top_term,top_coef)]\n",
    "\n",
    "def fine_tune_model(tweets, labels,best_result):\n",
    "    param_grid = {'alpha':list(np.arange(0.0,5.0,0.05)),'fit_prior':[True,False]}\n",
    "    tokens_list = [tokenize(tweet,keep_internal_punct = best_result['punct'],\n",
    "                           collapse_url = best_result['url'],collapse_mention = best_result['mention']) \n",
    "                   for tweet in tweets]\n",
    "    feature_list = list(best_result['features'])\n",
    "    X, vocab = vectorize(tokens_list,feature_list,best_result['min_freq'])\n",
    "    clf = MultinomialNB()\n",
    "    ran_search = RandomizedSearchCV(clf,param_grid,cv = 5, scoring = 'accuracy')\n",
    "    ran_search.fit(X,labels)\n",
    "    return ran_search.best_params_\n",
    "\n",
    "def main():\n",
    "    print('Reading and labeling raw tweets...')\n",
    "    male_tweets = list(set(get_tweet_data('maletweets.txt')))\n",
    "    female_tweets = list(set(get_tweet_data('femaletweets.txt')))\n",
    "    tweets, labels = assign_label_to_tweet(male_tweets,female_tweets)\n",
    "    print('\\nSampling train/test set using Stratified Sampling...')\n",
    "    X_train,X_test,y_train,y_test = get_train_test(tweets,labels)\n",
    "    print('\\nEvaluating over multiple preprocessing settings...')\n",
    "    feature_fns = [token_features,token_pair_features]#,lexicon_features]\n",
    "    results = eval_all_combinations(X_train,y_train,\n",
    "                                    [True,False],\n",
    "                                    [True, False],\n",
    "                                    [True,False],\n",
    "                                    feature_fns,\n",
    "                                    list(range(1,10)))\n",
    "    best_result = results[0]\n",
    "    worst_result = results[-1]\n",
    "    #Printing\n",
    "    print('\\nBest cross-validation result:\\n{}'.format(str(best_result)))\n",
    "    print('Worst cross-validation result:\\n{}'.format(str(worst_result)))\n",
    "    print('\\nMean Accuracy per setting: ')\n",
    "    print('\\n'.join(['%s: %.5f' % (s,v) for v,s in mean_accuracy_per_setting(results)]))\n",
    "    \n",
    "    print('\\nFine-tuning Multinomial Naive Bayes...')\n",
    "    best_params = fine_tune_model(X_train,y_train,best_result)\n",
    "    clf, vocab, train_prediction = fit_best_classifier(X_train, y_train, best_result,best_params)\n",
    "    print('Accuracy of fine-tuned MNB on train set: {}'.format(str(accuracy_score(y_train,train_prediction))))\n",
    "    test_prediction = predict_test_data(clf, X_test, best_result,vocab)\n",
    "    print('Accuracy of fine-tuned MNB on test set: {}'.format(str(accuracy_score(y_test,test_prediction))))\n",
    "    \n",
    "    # Print top coefficients per class.\n",
    "    print('\\nTOP COEFFICIENTS PER CLASS:')\n",
    "    print('Top words used by female critic:')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 0, 10, vocab)]))\n",
    "    print('\\Top words used by male critic')\n",
    "    print('\\n'.join(['%s: %.5f' % (t,v) for t,v in top_coefs(clf, 1, 10, vocab)]))\n",
    "    \n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
